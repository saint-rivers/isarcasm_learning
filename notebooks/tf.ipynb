{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/isarcasm/isarc_train.csv\")\n",
    "test = pd.read_csv(\"../data/isarcasm/isarc_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    # num_labels=2, \n",
    "    # loss_function_params={\"weight\": [0.75, 0.25]},\n",
    "    # model_max_length=512\n",
    "    )\n",
    "model = transformers.RobertaForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([[    0,  1039,   100,  ...,     1,     1,     1],\n",
       "         [    0,  1039, 20780,  ...,     1,     1,     1],\n",
       "         [    0,   243,   269,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0, 21518,   631,  ...,     1,     1,     1],\n",
       "         [    0, 45424,    70,  ...,     1,     1,     1],\n",
       "         [    0, 19854,  3056,  ...,     1,     1,     1]]),\n",
       " 'mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'label': tensor([0, 1, 1,  ..., 0, 0, 0]),\n",
       " 'label_ids': tensor([0, 1, 1,  ..., 0, 0, 0])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['tweet', 'sarcastic']]\n",
    "enc_train = tokenizer(\n",
    "    text=train['tweet'].values.tolist(), \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    return_tensors = 'pt',\n",
    "    # return_attention_mask=True,\n",
    "    # return_token_type_ids=True\n",
    "    )\n",
    "enc_label = torch.tensor(train['sarcastic'])\n",
    "\n",
    "enc_train = {\n",
    "    'ids': enc_train['input_ids'],\n",
    "    'mask': enc_train['attention_mask'],\n",
    "    'label': enc_label,\n",
    "    'label_ids': enc_label\n",
    "}\n",
    "enc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
