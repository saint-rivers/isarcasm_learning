{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dayan/projects/sarc_detection/twitter_scrape/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tokenizers\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"FacebookAI/roberta-base\"\n",
    "\n",
    "# df = df[1:10]\n",
    "class SarcasmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "## Test Dataset\n",
    "class SarcasmTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "    \n",
    "    \n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(labels, pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy,\"f1_score\":f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, add_prefix_space=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/isarcasm2022.csv\")\n",
    "df = df[['tweet', 'sarcastic']]\n",
    "df.dropna(inplace=True)\n",
    "    \n",
    "x_train, x_val, y_train, y_val = train_test_split(df['tweet'], df['sarcastic'], test_size=0.2, random_state=34)\n",
    "\n",
    "x_train = x_train.values.tolist()\n",
    "y_train = y_train.values.tolist() \n",
    "x_val = x_val.values.tolist()\n",
    "y_val = y_val.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2773"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   787,   100, 29631,  9335, 28908,  3006,  1952,    33,    10,\n",
       "           169,     9,   442,    82,  1032,   101,  7105,     7,    28,  2638,\n",
       "             8,  5324,    30,   106, 50121, 50118, 50121, 50118,   243,    18,\n",
       "           761,     9,  3444,     6,    11,    10, 42647,    62,   169,   787,\n",
       "         20780, 41925, 10494, 16911,    85,   269, 19230,   162,    77,  2948,\n",
       "         16511,  1115,     8, 18241,   239,   462, 38361,   130,    50,   237,\n",
       "           498,   228,  1040,    77, 10032,   111, 11253,    86,  1052,   328,\n",
       "           849, 33939,    38,  2813,   939,  2638,   127,   809,   787,  3952,\n",
       "           293, 12010,   787, 22026, 28612,  2596,  3303,  5314,    19,    70,\n",
       "            14,    37,    26,    19,     5,  1198,  9578,   139,    14,  6321,\n",
       "             8,  3701,   989,   137,     5,    94,     9,     5,  3517,   314,\n",
       "         17220,  6025,    21, 15478,    18,  1307,  5849,    20,  2105,    12,\n",
       "         21693,   841,    11,    70,     9,   209, 25816,     4, 11601,    62,\n",
       "           486,     4, 12602,  6801,    16,    10,  1050,   145,     4, 32775,\n",
       "           328,  2615,    75,   244,    53,   619,    10,   828, 34449,   160,\n",
       "            19,  8281,    71,    42,     4,    85,    18,  2105,    37,  1072,\n",
       "             7,   989,    53,    37,  1419,    10,  1355,     4,    91,   630,\n",
       "            75,   120,     7,  3211,    10,    39,  8628,  2564,     8,   120,\n",
       "            10,  2937,    14,    16, 10625, 20134, 21584,     4,    85,    18,\n",
       "            41, 46046,  4735,  1307,  9951,     7,   310,  1037, 18904,     4,\n",
       "         10812,    62,     4,   114,  1236,   293,   687,    21, 38952,   456,\n",
       "           939,    74,   900,    13,   123,   993,  1972,   269,    32,  4220,\n",
       "          1827,  8103, 16948,  8384,    38, 24909,   393,    33,    10,  1607,\n",
       "            19,   110,   313,    59,   562,  4009,     6,   562,  2997,    50,\n",
       "           519,  1159,   142,    47,    17,    27,   890,   393,   120,     5,\n",
       "          1948, 18636,  9264,  6569, 10470, 18164,  6569,  9357,  4394,    17,\n",
       "          8384, 38718,  7471, 12605, 28386,     7,   216,    70,    42,   750,\n",
       "          3422,    16,  6908,  3549,   162,  8432,   127,    78,   738,     4,\n",
       "         30895,  1256,   205,     4, 17161,   102,   619,   101,  2159,   103,\n",
       "          3709,   785,     4,    20,   738,   399,    75,   269,  2581,     4,\n",
       "           125,    99,    40,  2581,    16,   114,    38,   218,    75,   185,\n",
       "          2093,     9,     5,  3709,   558,   432,   164,    15,   235,   122,\n",
       "            13,    10,  1804,    86, 41381, 14932,     8,     5, 13645,    36,\n",
       "         40954,   322,  2700, 12134, 16994,   822,     9,    70,    86,     4,\n",
       "            20,  7362,   661,   174,   162,    14,    38,   956,    10, 25001,\n",
       "             7,     5,  1886, 24626,   190,   600,    38,    33,    57,    11,\n",
       "             8,    66,     9,    14,  1494,   187,    38,    21,  2421,    36,\n",
       "          8877,    21,  1593,    43,    38,    21,    95,  2828,    15,     5,\n",
       "          1028,   101, 26964, 10172, 31193, 11936, 31193, 10172,  8578, 38713,\n",
       "            38,   240,  2393,  9268,    25,    10,  4910,    15,  1275, 13207,\n",
       "           787,   523, 38142,   257,   787, 23031,  5182,   152,    16,    10,\n",
       "          2247,  1579,    47,    32, 26991,  4048,     8,    38,    33,     5,\n",
       "            62,  7877,  2098,    13,    47,    13,  2057,    24,    70,    66,\n",
       "            89,     4,    38, 40455,  3842, 11932,  3842,  1941,  6362, 11083,\n",
       "          1437, 16506, 11733,     7,   256,   947,  3914,   131,   104,   689,\n",
       "            40,    28,     5,  6053,     9,   127,   183,   423,     6,    64,\n",
       "            17,    27,    90,  2067, 31193, 14285,  6569,  9357,  2023,  6569,\n",
       "         48278, 11812,    10,  2335,    21,   881,  4507,   352,     5,   275,\n",
       "             8,  2373,   631,    38,    17,    27,   548,   655,   626,  7586,\n",
       "          7424,    17,    27,    90,  4744,   301,   396,    69,     8,    38,\n",
       "            64,    17,    27,    90,   697,   396,    69,  7586,  5293,  1472,\n",
       "           116,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings = tokenizer(text=x_train, truncation=True, padding=True, is_split_into_words=True, return_tensors='pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
